---
layout: post
title: Why Weak AI Is Human's Slave
date: 2017-05-07
tags: AI
category: [Tech]
comments: true

---

> Weak AI's standard of being right or wrong comes from humans only.

If you Google "slave", you will find its definition being "a person who is the legal property of another and is forced to obey them". Currently, the [weak AI](https://en.wikipedia.org/wiki/Weak_AI) meets both conditions. The former condition is easier to see, while the latter one is not that easily seen.

To understand the latter condition, we need to think about what makes it to be "obey someone". In my opinion, "I obey someone" means I have to use someone else's standard as my standard, and that person has absolute authority to decide if the output that I produce is acceptable or not.

This is applicable to the weak AI. We human beings are the absolute authority to decide if the results generated by the AI are correct or not according to we humans' standard. If we say the result is wrong, the AI must agree with that and go back to change its internal logic in order to try to come to the right result the next time.

However, [the AI that I'm trying to build](http://yaobinwen.github.io/archive/2017/05/06/AI-Mindset-Build-my-own-AI/) is not slave of human beings. Although they both need training by humans, the training is done towards different objects. In machine learning, the training focuses on judging the correctness of the output produced by the machines, and the judgments on the final output is where humans are able to use their absolute power over machines. However, in my AI system, the training is done towards the knowledge of the AI, then it's the AI itself that uses the knowledge and reasoning to come to the result. Some people can say that the result is "wrong" according to their standard, but the AI can ignore that and still produce the same result the next time when given the same input. These people can try to modify the knowledge that the AI has, but their modification may not work because the AI considers ALL the possible input sources to judge if a piece of knowledge should be built or not. In other words, the AI can still hold the knowledge as long as a lot of other sources - not necessarily humans but the other sources from the external environment - believe the knowledge is true even if these people disagree with that.

Viewing from this perspective, this kind of AI is no longer humans' slave. Humans can of course make a great impact to the AI but they no longer have absolute power over it, because the knowledge sources of this AI can be the sensors that bypass the humans and directly communicate with the external environment.
